#!/usr/bin/env python3
'''
autocombine schism hotstart in parallel
'''
from pylib import *
import time

#-----------------------------------------------------------------------------
#Input
#-----------------------------------------------------------------------------
stacks=None #[list of stacks starting from 0 | None means all stacks]
idelete_raw=0  #delete uncombine results

#resource requst 
walltime='00:10:00' 
#qnode='x5672'; nnode=2; ppn=8      #hurricane, ppn=8
qnode='bora'; nnode=2; ppn=20      #bora, ppn=20
#qnode='vortex'; nnode=2; ppn=12    #vortex, ppn=12
#qnode='femto'; nnode=2; ppn=12     #femto,ppn=32
#qnode='potomac'; nnode=4; ppn=8    #ches, ppn=12
#qnode='james'; nnode=5; ppn=20     #james, ppn=20
#qnode='skylake'; nnode=2; ppn=36   #viz3,skylake, ppn=36
#qnode='haswell'; nnode=2; ppn=2    #viz3,haswell, ppn=24,or 28

ibatch=1; scrout='screen2.out'; bdir=os.path.abspath(os.path.curdir)
jname='cmb_{}'.format(os.path.basename(bdir)); code='~/bin/combine_hotstart7.'+qnode
#-----------------------------------------------------------------------------
#on front node: 1). submit jobs first (qsub), 2) running parallel jobs (mpirun) 
#-----------------------------------------------------------------------------
if ibatch==0: os.environ['job_on_node']='1'; os.environ['bdir']=bdir #run locally
if os.getenv('job_on_node')==None:
   if os.getenv('param')==None: fmt=0; bcode=sys.argv[0]
   if os.getenv('param')!=None: fmt=1; bdir,bcode=os.getenv('param').split(); os.chdir(bdir)
   scode=get_hpc_command(bcode,bdir,jname,qnode,nnode,ppn,walltime,scrout,fmt=fmt)
   print(scode); os.system(scode); os._exit(0)

#-----------------------------------------------------------------------------
#on computation node
#-----------------------------------------------------------------------------
bdir=os.getenv('bdir'); os.chdir(bdir) #enter working dir
comm=MPI.COMM_WORLD; nproc=comm.Get_size(); myrank=comm.Get_rank()
if myrank==0: t0=time.time()

#-----------------------------------------------------------------------------
#do MPI work on each core
#-----------------------------------------------------------------------------
#get stacks
iprocs_all=sort(array([i.split('_')[2].split('.')[0] for i in os.listdir('outputs') if i.startswith('hotstart_0000')]).astype('int'))
iprocs=iprocs_all if (stacks is None) else iprocs_all[array(stacks)]

#combine hotstart on each rank
iproc=[iprocs[i] for i in arange(len(iprocs)) if i%nproc==myrank]
gb=getglob('outputs/local_to_global_0000')
for i in iproc:
    os.system("cd outputs; {} -i {} -p {} -t {}; mv hotstart_it={}.nc hotstart.nc_{}".format(code,i,gb.nproc,gb.ntracers,i,i))
    print('finished combining stack={}, on myrank={}'.format(i,myrank)); sys.stdout.flush()
    if idelete_raw!=0: os.system('cd outputs; rm hotstart_????_{}.nc'.format(i))

#-----------------------------------------------------------------------------
#finish MPI jobs
#-----------------------------------------------------------------------------
comm.Barrier()
if myrank==0: dt=time.time()-t0; print('total time used: {} s'.format(dt)); sys.stdout.flush()
sys.exit(0) if qnode in ['bora'] else os._exit(0)
