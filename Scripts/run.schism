#!/usr/bin/env python3
'''
  atuo script to submit SCHISM batch jobs on sciclone/james
  note: environmental variables "run_schism" is used to load correct modules 
'''
import os,sys

#-----------------------------------------------------------------------------
#Input
#-----------------------------------------------------------------------------
runs=[]  #used to submit multiple jobs
#code='./pschism_WW_COSINE_TVD-VL.b2375daf'
#code='./pschism_FEMTO_COSINE_TVD-VL.b2375daf'
#code='./pschism_BORA_COSINE_TVD-VL.b2375daf'
code='./pschism_MAINE_VL_COS.603f0b4a'
walltime='72:00:00'

#resource requst 
#qnode='bora'; nnode=1; ppn=1      #bora, ppn=20
#qnode='vortex'; nnode=10; ppn=12   #vortex, ppn=12
#qnode='x5672'; nnode=1; ppn=2      #hurricane, ppn=8
#qnode='potomac'; nnode=4; ppn=12    #ches, ppn=12
#qnode='james'; nnode=2; ppn=20     #james, ppn=20
#qnode='femto'; nnode=1; ppn=32      #femto,ppn=32, not working yet
#qnode='skylake'; nnode=1; ppn=36    #viz3,skylake, ppn=36
qnode='haswell'; nnode=5; ppn=12   #viz3,haswell, ppn=24,or 28

#-----------------------------------------------------------------------------
#pre-processing
#-----------------------------------------------------------------------------
bdir=os.path.abspath(os.path.curdir); nproc=nnode*ppn

#-----------------------------------------------------------------------------
#on front node; submit jobs in this section
#-----------------------------------------------------------------------------
if os.getenv('run_schism')==None and os.getenv('job_on_node')==None:
    args=sys.argv; jname=os.path.basename(bdir)

    #submit job on node
    if qnode=='femto':
       scode='sbatch --export=run_schism="1" -J {} -N {} -n {} -t {} {}'.format(jname,nnode,ppn,walltime,args[0])
    elif qnode in ['haswell','skylake']:
       #scode='qsub {} -v run_schism="1", -N {} -j oe -l nodes={}:{}:ppn={} -l walltime={}'.format(args[0],jname,nnode,qnode,ppn,walltime)
       scode='qsub {} -v run_schism="1", -N {} -j oe -l procs={} -l walltime={}'.format(args[0],jname,nproc,walltime)
    else:
       scode='qsub {} -v run_schism="1", -N {} -j oe -l nodes={}:{}:ppn={} -l walltime={}'.format(args[0],jname,nnode,qnode,ppn,walltime)
    print(scode); os.system(scode); sys.stdout.flush()
    os._exit(0)

#-----------------------------------------------------------------------------
#still on front node, but in batch mode; running jobs in this section
#-----------------------------------------------------------------------------
if os.getenv('run_schism')!=None and os.getenv('job_on_node')==None:
    if qnode=='femto': 
       bdir=os.getenv('SLURM_SUBMIT_DIR')
    else:
       bdir=os.getenv('PBS_O_WORKDIR')
    os.chdir(bdir)
    
    if qnode=='femto':
       rname='run_sch'; fid=open(rname,'w+'); fid.write('#!/usr/bin/tcsh\nsrun {} >& screen.out'.format(code)); fid.close()
       rcode='chmod a+x {}; ./{}'.format(rname,rname)
    elif qnode in ['james','x5672','bora']:
       rcode="mvp2run -v -C 0.05 {} >& screen.out".format(code)
    elif qnode in ['skylake','haswell']:
       rcode="mpirun {} >& screen.out".format(code)
    print(rcode); os.system(rcode); sys.stdout.flush()
    os._exit(0)

